{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4fQDuWtascOa5pT+UCal4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RashikaTyagi01/Rashika-UML501/blob/main/mlAssignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Write a Python program to scrape all available books from the website\n",
        "(https://books.toscrape.com/) Books to Scrape – a live site built for practicing scraping (safe,\n",
        "legal, no anti-bot). For each book, extract the following details:\n",
        "1. Title\n",
        "2. Price\n",
        "3. Availability (In stock / Out of stock)\n",
        "4. Star Rating (One, Two, Three, Four, Five)\n",
        "Store the scraped results into a Pandas DataFrame and export them to a CSV file named\n",
        "books.csv.\n",
        "(Note: Use the requests library to fetch the HTML page. Use BeautifulSoup to parse and extract\n",
        "book details and handle pagination so that books from all pages are scraped)"
      ],
      "metadata": {
        "id": "0jSpkMRi_Muh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RzVCcyyA8v3v"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
        "books = []\n",
        "page = 1\n",
        "while True:\n",
        "   url = base_url.format(page)\n",
        "   response = requests.get(url)\n",
        "   if response.status_code != 200:\n",
        "        break\n",
        "   soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "   book_items = soup.find_all(\"article\", class_=\"product_pod\")\n",
        "   if not book_items:\n",
        "        break\n",
        "   for item in book_items:\n",
        "    title = item.h3.a[\"title\"]\n",
        "    price = item.find(\"p\", class_=\"price_color\").text.strip()\n",
        "    availability = item.find(\"p\", class_=\"instock availability\").text.strip()\n",
        "    rating=item.p[\"class\"][1]\n",
        "    books.append([title, price, availability, rating])\n",
        "\n",
        "   page+=1\n",
        "df=pd.DataFrame(books, columns=[\"Title\", \"Price\", \"Availability\", \"Star Rating\"])\n",
        "df.to_csv(\"books.csv\", index=False)\n",
        "print(df.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj5lsEQS6ZYt",
        "outputId": "f924ec79-58cd-4289-8a33-43024be233a0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                   Title    Price Availability Star Rating\n",
            "0                   A Light in the Attic  Â£51.77     In stock       Three\n",
            "1                     Tipping the Velvet  Â£53.74     In stock         One\n",
            "2                             Soumission  Â£50.10     In stock         One\n",
            "3                          Sharp Objects  Â£47.82     In stock        Four\n",
            "4  Sapiens: A Brief History of Humankind  Â£54.23     In stock        Five\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Write a Python program to scrape the IMDB Top 250 Movies list\n",
        "(https://www.imdb.com/chart/top/) . For each movie, extract the following details:\n",
        "1. Rank (1–250)\n",
        "2. Movie Title\n",
        "3. Year of Release\n",
        "4. IMDB Rating\n",
        "Store the results in a Pandas DataFrame and export it to a CSV file named imdb_top250.csv.\n",
        "(Note: Use Selenium/Playwright to scrape the required details from this website)"
      ],
      "metadata": {
        "id": "K4hmktcj_Uyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "gRqh8KFLBq13",
        "outputId": "3ee5aedd-00d8-43a4-c0f6-a2394b389314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.35.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.8.3)\n",
            "Collecting typing_extensions~=4.14.0 (from selenium)\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.35.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, typing_extensions, outcome, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.35.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.14.1 wsproto-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing_extensions"
                ]
              },
              "id": "626aae3b4e294b8ca7bd14d0bf14ebb0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import time\n",
        "\n",
        "chrome_opts = Options()\n",
        "chrome_opts.add_argument(\"--headless\")\n",
        "chrome_opts.add_argument(\"--no-sandbox\")\n",
        "chrome_opts.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_opts.add_argument(\n",
        "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "    \"Chrome/115.0.0.0 Safari/537.36\"\n",
        ")\n",
        "browser = webdriver.Chrome(options=chrome_opts)\n",
        "browser.get(\"https://www.imdb.com/chart/top/\")\n",
        "time.sleep(5)\n",
        "film_list = []\n",
        "movie_cards = browser.find_elements(By.CSS_SELECTOR, \".ipc-metadata-list-summary-item\")\n",
        "for rank, card in enumerate(movie_cards, start=1):\n",
        "    try:\n",
        "        name = card.find_element(By.CSS_SELECTOR, \"h3\").text\n",
        "        release_year = card.find_element(By.CSS_SELECTOR, \".cli-title-metadata-item\").text\n",
        "        score = card.find_element(By.CSS_SELECTOR, \".ipc-rating-star--imdb\").text.split()[0]\n",
        "        film_list.append([rank, name, release_year, score])\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping a card due to error: {e}\")\n",
        "browser.quit()\n",
        "imdb_table = pd.DataFrame(film_list, columns=[\"Position\", \"Movie Title\", \"Release Year\", \"Rating\"])\n",
        "imdb_table.to_csv(\"imdb_top250.csv\", index=False)\n",
        "print(imdb_table.head())"
      ],
      "metadata": {
        "id": "8Z61vYsIDOdy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cef044b-091c-405d-92c5-bbc3292c4166"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Position                  Movie Title Release Year Rating\n",
            "0         1  1. The Shawshank Redemption         1994    9.3\n",
            "1         2             2. The Godfather         1972    9.2\n",
            "2         3           3. The Dark Knight         2008    9.1\n",
            "3         4     4. The Godfather Part II         1974    9.0\n",
            "4         5              5. 12 Angry Men         1957    9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Write a Python program to scrape the weather information for top world cities from the\n",
        "given website (https://www.timeanddate.com/weather/) . For each city, extract the following\n",
        "details:\n",
        "1. City Name\n",
        "2. Temperature\n",
        "3. Weather Condition (e.g., Clear, Cloudy, Rainy, etc.)\n",
        "Store the results in a Pandas DataFrame and export it to a CSV file named weather.csv."
      ],
      "metadata": {
        "id": "dQkeSTUf_Yo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url = \"https://www.timeanddate.com/weather/\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "table = soup.find(\"table\")\n",
        "if table is None:\n",
        "    print(\"No table found\")\n",
        "data = []\n",
        "for row in table.find_all(\"tr\"):\n",
        "    cols = row.find_all(\"td\")\n",
        "    if len(cols) >= 4:\n",
        "        city = cols[0].get_text(strip=True)\n",
        "        temperature = cols[2].get_text(strip=True)\n",
        "        condition = cols[3].get_text(strip=True)\n",
        "        data.append({\"City\": city, \"Temperature\": temperature, \"Condition\": condition})\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(\"weather.csv\", index=False)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMePTIsS-8uG",
        "outputId": "ed4950b3-9797-426a-994f-3179ae4d8471"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          City Temperature Condition\n",
            "0        Accra                 79 °F\n",
            "1  Addis Ababa                 64 °F\n",
            "2     Adelaide                 55 °F\n",
            "3      Algiers                 78 °F\n",
            "4       Almaty                 55 °F\n"
          ]
        }
      ]
    }
  ]
}