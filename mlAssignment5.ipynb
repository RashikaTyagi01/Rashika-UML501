{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmWC1iMCRS6LHisY+tl1zo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RashikaTyagi01/Rashika-UML501/blob/main/mlAssignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. (Based on Step-by-Step Implementation of Ridge Regression using Gradient\n",
        "Descent Optimization)\n",
        "Generate a dataset with atleast seven highly correlated columns and a target variable.\n",
        "Implement Ridge Regression using Gradient Descent Optimization. Take different\n",
        "values of learning rate (such as 0.0001,0.001,0.01,0.1,1,10) and regularization\n",
        "parameter (10-15,10-10,10-5\n",
        ",10- 3\n",
        ",0,1,10,20). Choose the best parameters for which ridge\n",
        "regression cost function is minimum and R2_score is maximum."
      ],
      "metadata": {
        "id": "tz5rrTWBf86W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fZepQT0OVaob"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "n = 500\n",
        "z = np.random.randn(n)\n",
        "X = np.column_stack([z + 0.01 * np.random.randn(n) for _ in range(7)])\n",
        "X = np.column_stack([X, 0.5 * z + 0.2 * np.random.randn(n)])\n",
        "true_weights=np.array([-3, -2, -1, 0, 0.5, 2, 3, 4],dtype=float)\n",
        "target=X.dot(true_weights) + 0.5 * np.random.randn(n)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.25, random_state=1)\n",
        "mean = X_train.mean(axis=0)\n",
        "std = X_train.std(axis=0)\n",
        "std = np.where(np.isfinite(std) & (std > 0), std, 1.0)\n",
        "X_train=(X_train - mean)/std\n",
        "X_test=(X_test - mean)/std\n",
        "\n",
        "def ridge(X,y,alpha,lmbda,epochs=2000):\n",
        "   X = X.astype(np.float64)\n",
        "   y = y.astype(np.float64)\n",
        "   m, n = X.shape\n",
        "   weights = np.zeros(n, dtype=np.float64)\n",
        "   bias = 0.0\n",
        "   for _ in range(epochs):\n",
        "        predictions = X.dot(weights) + bias\n",
        "        error = predictions - y\n",
        "        grad_w = (2/m)*(X.T.dot(error))+2*lmbda*weights\n",
        "        grad_b = (2/m)*error.sum()\n",
        "        weights -= alpha * grad_w\n",
        "        bias -= alpha * grad_b\n",
        "        if not (np.isfinite(weights).all() and np.isfinite(bias)):\n",
        "            return None\n",
        "   return weights, bias\n",
        "\n",
        "learning_rates=[0.0001,0.001,0.01,0.1,1,10]\n",
        "r_parameter=[1e-15, 1e-10, 1e-5, 1e-3, 0, 1, 10, 20]\n",
        "best_r2 = -1\n",
        "best_parameters = None\n",
        "for lr in learning_rates:\n",
        "    for lam in r_parameter:\n",
        "        result = ridge(X_train, y_train, lr, lam)\n",
        "        if result is None:\n",
        "            continue\n",
        "        weights, bias = result\n",
        "        predictions = X_test.dot(weights) + bias\n",
        "        if not np.isfinite(predictions).all():\n",
        "            continue\n",
        "        r2 = r2_score(y_test, predictions)\n",
        "        if r2 > best_r2:\n",
        "            best_r2 = r2\n",
        "            best_parameters = (lr, lam, r2)\n",
        "\n",
        "print(\"Best Learning Rate, Regularization parameter, R2 =\", best_parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nheUFcM9VnD9",
        "outputId": "509c21f5-f150-43bf-c657-7a76beacccfd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Learning Rate, Regularization parameter, R2 = (0.1, 0, 0.9228808570261661)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:52: RuntimeWarning: overflow encountered in reduce\n",
            "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:52: RuntimeWarning: invalid value encountered in reduce\n",
            "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Load the Hitters dataset from the following link\n",
        "https://drive.google.com/file/d/1qzCKF6JKKMB0p7ul_lLy8tdmRk3vE_bG/view?usp=sharing\n",
        "(a) Pre-process the data (null values, noise, categorical to numerical encoding)\n",
        "(b) Separate input and output features and perform scaling\n",
        "(c) Fit a Linear, Ridge (use regularization parameter as 0.5748), and LASSO (use\n",
        "regularization parameter as 0.5748) regression function on the dataset.\n",
        "(d) Evaluate the performance of each trained model on test set. Which model\n",
        "performs the best and Why?"
      ],
      "metadata": {
        "id": "yqFoqrXhgbCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error"
      ],
      "metadata": {
        "id": "-gINkJH2ETnp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/Hitters.csv')\n",
        "df=df.dropna()\n",
        "df=pd.get_dummies(df,columns=['League','Division','NewLeague'],drop_first=True)\n",
        "X = df.drop(\"Salary\", axis=1)\n",
        "y = df[\"Salary\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "sc = StandardScaler()\n",
        "X_train_scaled = sc.fit_transform(X_train)\n",
        "X_test_scaled = sc.transform(X_test)\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "ridge = Ridge(alpha=0.5748)\n",
        "ridge.fit(X_train_scaled, y_train)\n",
        "lasso = Lasso(alpha=0.5748, max_iter=5000)\n",
        "lasso.fit(X_train_scaled, y_train)\n",
        "y_pred_lr = lr.predict(X_test_scaled)\n",
        "y_pred_ridge = ridge.predict(X_test_scaled)\n",
        "y_pred_lasso = lasso.predict(X_test_scaled)\n",
        "\n",
        "print(\"Linear Regression R2:\", r2_score(y_test, y_pred_lr))\n",
        "print(\"Ridge Regression R2:\", r2_score(y_test, y_pred_ridge))\n",
        "print(\"Lasso Regression R2:\", r2_score(y_test, y_pred_lasso))\n",
        "\n",
        "print(\"\\nLinear MSE:\", mean_squared_error(y_test, y_pred_lr))\n",
        "print(\"Ridge MSE:\", mean_squared_error(y_test, y_pred_ridge))\n",
        "print(\"Lasso MSE:\", mean_squared_error(y_test, y_pred_lasso))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_cgF4DyfxMy",
        "outputId": "a106d0f9-6aa8-4617-aacd-7fde524d649a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression R2: 0.290745185579814\n",
            "Ridge Regression R2: 0.30003596988293446\n",
            "Lasso Regression R2: 0.29962566098567167\n",
            "\n",
            "Linear MSE: 128284.34549672354\n",
            "Ridge MSE: 126603.90264424692\n",
            "Lasso MSE: 126678.11604014733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Cross Validation for Ridge and Lasso Regression\n",
        "Explore Ridge Cross Validation (RidgeCV) and Lasso Cross Validation (LassoCV)\n",
        "function of Python. Implement both on Boston House Prediction Dataset (load_boston\n",
        "dataset from sklearn.datasets)."
      ],
      "metadata": {
        "id": "T_2rxsl5myIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.metrics import r2_score"
      ],
      "metadata": {
        "id": "FsUfKr9Fge_p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "s=StandardScaler()\n",
        "X_train_scaled=s.fit_transform(X_train)\n",
        "X_test_scaled=s.transform(X_test)\n",
        "alphas = np.logspace(-3, 3, 50)\n",
        "ridge_cv = RidgeCV(alphas=alphas, cv=5)\n",
        "ridge_cv.fit(X_train_scaled, y_train)\n",
        "lasso_cv = LassoCV(alphas=None, cv=5, max_iter=10000, random_state=42)\n",
        "lasso_cv.fit(X_train_scaled, y_train)\n",
        "print(\"Ridge best alpha:\", ridge_cv.alpha_)\n",
        "print(\"Ridge R2:\", r2_score(y_test, ridge_cv.predict(X_test_scaled)))\n",
        "print(\"Lasso best alpha:\", lasso_cv.alpha_)\n",
        "print(\"Lasso R2:\", r2_score(y_test, lasso_cv.predict(X_test_scaled)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSQoDcj5gt8R",
        "outputId": "6d54afb7-ecbc-4049-d6a3-a1da19731928"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge best alpha: 2.6826957952797246\n",
            "Ridge R2: 0.6679648354383835\n",
            "Lasso best alpha: 0.006863892263379668\n",
            "Lasso R2: 0.6683883969336302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Multiclass Logistic Regression: Implement Multiclass Logistic Regression (step-by step)\n",
        "on Iris dataset using one vs. rest strategy?"
      ],
      "metadata": {
        "id": "Z0UIhS99m3Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n"
      ],
      "metadata": {
        "id": "KuDUc8Xzm7wR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris_data=load_iris()\n",
        "X=iris_data.data\n",
        "y=iris_data.target\n",
        "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "sc=StandardScaler()\n",
        "X_train=sc.fit_transform(X_train)\n",
        "X_test=sc.transform(X_test)\n",
        "model=OneVsRestClassifier(LogisticRegression(max_iter=2000))\n",
        "model.fit(X_train, y_train)\n",
        "y_pred=model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zycXF1JpDzE",
        "outputId": "14cb73e0-a278-4999-ad61-985531e265c8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9111111111111111\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        16\n",
            "           1       0.94      0.83      0.88        18\n",
            "           2       0.77      0.91      0.83        11\n",
            "\n",
            "    accuracy                           0.91        45\n",
            "   macro avg       0.90      0.91      0.91        45\n",
            "weighted avg       0.92      0.91      0.91        45\n",
            "\n"
          ]
        }
      ]
    }
  ]
}